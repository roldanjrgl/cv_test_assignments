{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment 1.2 - Computer Vision CSCI-GA.2272-001",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roldanjrgl/cv_test_assignments/blob/master/Copy_of_Assignment_1_2_Computer_Vision_CSCI_GA_2272_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN5lM2csj9v7"
      },
      "source": [
        "# Computer Vision CSCI-GA.2272-001 Assignment 1, part 2.\n",
        "\n",
        "Fall 2021 semester.\n",
        "\n",
        "Due date: **September 30th 2021.**\n",
        "\n",
        "## Introduction\n",
        "This assignment requires you to participate in a Kaggle competition with the rest of the class on the The German Traffic Sign Recognition Benchmark [http://benchmark.ini.rub.de/?section=gtsrb&subsection=news]. The objective is to produce a model that gives the highest possible accuracy on the test portion of this dataset. You can register for the competition using the private link: https://www.kaggle.com/c/nyu-computer-vision-csci-ga2271-2021/overview.\n",
        "\n",
        "Skeleton code is provided in the colab below. This contains code for training a simple default model and evaluating it on the test set. The evaluation script produces a file gtsrb_kaggle.csv that lists the IDs of the test set images, along with their predicted label. This file should be uploaded to the Kaggle webpage, which will then produce a test accuracy score. \n",
        "\n",
        "Your goal is to implement a new model architecture that improves upon the baseline performance. You are free to implement any approach covered in class or from research papers. This part will count for 50% of the overall grade for assignment 1. This Grading will depend on your Kaggle performance and rank, as well as novelty of the architecture.  \n",
        "\n",
        "## Rules\n",
        "You should make a copy of this Colab (File->Save a copy in Drive). Please start the assignment early and don’t be afraid to ask for help from either the TAs or myself. You are allowed to collaborate with other students in terms discussing ideas and possible solutions. However you code up the solution yourself, i.e. you must write your own code. Copying your friends code and just changing all the names of the variables is not allowed! You are not allowed to use solutions from similar assignments in courses from other institutions, or those found elsewhere on the web.\n",
        "Your solutions should be submitted via the Brightspace system. This should include a brief description (in the Colab) explaining the model architectures you explored, citing any relevant papers or techniques that you used. You should also include convergence plots of training accuracy vs epoch for relevant models. \n",
        "\n",
        "### Important Details\n",
        "• You are only allowed eight (8) submissions to the Kaggle evaluation server per day. This is to prevent over-fitting on the test dataset. So be sure to start the assignment early!\n",
        "\n",
        "• You are NOT ALLOWED to use the test set labels during training in any way. Doing so will be regarded as cheating and penalized accordingly.\n",
        "\n",
        "• The evaluation metric is accuracy, i.e. the fraction of test set examples where the predicted label agrees with the ground truth label.\n",
        "\n",
        "• You should be able to achieve a test accuracy of at least 0.95. \n",
        "\n",
        "• *Extra important:* Please use your NYU NetID as your Kaggle username, so the TAs can figure out which user you are on the leaderboard. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktMeuH0DyPO8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pghx7ngowha0"
      },
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "\n",
        "1.  Download `dataset.zip` from to your local machine.\n",
        "2.  Unzip the file. You should see a `dataset` directory with three subfolders (`training,validation,testing`). \n",
        "3.  Go to Google Drive (on your NYU account) and make a directory `assign2_dataset` (New button --> New Folder).\n",
        "4.  Upload each of the three subfolders to it (New button --> Folder upload). \n",
        "5.  Run the code block below. It will ask for permission to mount your Google Drive (NYU account) so this colab can access it. Paste the authorization code into the box as requested. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0aPnIKXpWbN"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/'My Drive'/assign2_dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6jVfIVtrn5u"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z21UKj_bT--_"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size = 32\n",
        "momentum = 0.9\n",
        "lr = 0.01\n",
        "epochs = 10\n",
        "log_interval = 10\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_path=\"X.pt\", y_path=\"y.pt\"):\n",
        "\n",
        "        self.X = torch.load(X_path).squeeze(1)\n",
        "        self.y = torch.load(y_path).squeeze(1)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = MyDataset(X_path=\"train/X.pt\", y_path=\"train/y.pt\")\n",
        "val_dataset = MyDataset(X_path=\"validation/X.pt\", y_path=\"validation/y.pt\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd6W0pQRvZKO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeev4SoMvazV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nclasses = 43 # GTSRB has 43 classes\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(500, 50)\n",
        "        self.fc2 = nn.Linear(50, nclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 500)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty9TAvrdvf8C"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A5-OCgBvhXv"
      },
      "source": [
        "model = Net()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def validation():\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        output = model(data)\n",
        "        validation_loss += F.nll_loss(output, target, reduction=\"sum\").item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    validation()\n",
        "    model_file = 'model_' + str(epoch) + '.pth'\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    print('\\nSaved model to ' + model_file + '.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVN1f1p7w59X"
      },
      "source": [
        "# Evaluate and Submit to Kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BM5qP64w5zB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2bb3bcb-3fe5-4353-9419-e5b0999707fa"
      },
      "source": [
        "\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "outfile = 'gtsrb_kaggle.csv'\n",
        "\n",
        "output_file = open(outfile, \"w\")\n",
        "dataframe_dict = {\"Filename\" : [], \"ClassId\": []}\n",
        "\n",
        "test_data = torch.load('testing/test.pt')\n",
        "file_ids = pickle.load(open('testing/file_ids.pkl', 'rb'))\n",
        "model = Net() # TODO: load your model here, don't forget to put it on Eval mode !\n",
        "\n",
        "for i, data in enumerate(test_data):\n",
        "    data = data.unsqueeze(0)\n",
        "\n",
        "    output = model(data)\n",
        "    pred = output.data.max(1, keepdim=True)[1].item()\n",
        "    file_id = file_ids[i][0:5]\n",
        "    dataframe_dict['Filename'].append(file_id)\n",
        "    dataframe_dict['ClassId'].append(pred)\n",
        "\n",
        "df = pd.DataFrame(data=dataframe_dict)\n",
        "df.to_csv(outfile, index=False)\n",
        "print(\"Written to csv file {}\".format(outfile))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Written to csv file gtsrb_kaggle.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhSl_4kn6sox"
      },
      "source": [
        "# Submitting to Kaggle\n",
        "\n",
        "Now take this csv file, download it from your Google drive and then submit it to Kaggle to check performance of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvYRzHFc0mX_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}